{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Modèles\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Prétraitement et pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Modèles de validation\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "def load_data():\n",
    "    df_data = pd.read_csv('train.csv', parse_dates=['Date'])\n",
    "    df_stores = pd.read_csv('store.csv')\n",
    "    df = pd.merge(df_data, df_stores, on='Store', how='left')\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Prétraitement des données\n",
    "def preprocess_data(df):\n",
    "    # Filtrer les magasins ouverts et les ventes supérieures à zéro\n",
    "    df = df[(df['Open'] == 1) & (df['Sales'] > 0)]\n",
    "    \n",
    "    # Remplacer les valeurs manquantes\n",
    "    df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Convertir les colonnes catégoriques\n",
    "    categorical_cols = ['StoreType', 'Assortment', 'StateHoliday']\n",
    "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "    \n",
    "    # Ajouter des features temporelles\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
    "    df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # Créer des features de décalage (lags) pour les modèles ML\n",
    "    df.sort_values(['Store', 'Date'], inplace=True)\n",
    "    for lag in range(1, 8):\n",
    "        df[f'Sales_Lag_{lag}'] = df.groupby('Store')['Sales'].shift(lag)\n",
    "    \n",
    "    # Supprimer les lignes avec des valeurs manquantes après le décalage\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données\n",
    "def train_test_split(df, test_size=0.2):\n",
    "    # Nous allons prendre les dernières dates comme ensemble de test\n",
    "    train_index = int(len(df) * (1 - test_size))\n",
    "    df_train = df.iloc[:train_index]\n",
    "    df_test = df.iloc[train_index:]\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_split(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des features et de la cible\n",
    "features = [\n",
    "    'Store', 'DayOfWeek', 'Promo', 'Year', 'Month', 'Day', 'WeekOfYear', 'CompetitionDistance', \n",
    "    'Promo2', 'Sales_Lag_1', 'Sales_Lag_2', 'Sales_Lag_3', 'Sales_Lag_4', 'Sales_Lag_5', 'Sales_Lag_6', 'Sales_Lag_7',\n",
    "    'StoreType', 'Assortment', 'StateHoliday', 'IsWeekend'\n",
    "]\n",
    "target = 'Sales'\n",
    "\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "# Préparation du préprocesseur\n",
    "numerical_cols = ['Store', 'DayOfWeek', 'Promo', 'Year', 'Month', 'Day', 'WeekOfYear', 'CompetitionDistance', \n",
    "                  'Promo2', 'Sales_Lag_1', 'Sales_Lag_2', 'Sales_Lag_3', 'Sales_Lag_4', 'Sales_Lag_5', 'Sales_Lag_6', 'Sales_Lag_7', 'IsWeekend']\n",
    "categorical_cols = ['StoreType', 'Assortment', 'StateHoliday']\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/linear_regression_pipeline.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline de Régression Linéaire\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entraîner le modèle\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "joblib.dump(lr_pipeline, 'models/linear_regression_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/random_forest_pipeline.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline de Random Forest\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "joblib.dump(rf_pipeline, 'models/random_forest_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/xgboost_pipeline.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline de XGBoost\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Entraîner le modèle\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "joblib.dump(xgb_pipeline, 'models/xgboost_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner un modèle SARIMA pour un magasin spécifique\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def train_sarima(store_id):\n",
    "    store_data = df[df['Store'] == store_id].sort_values('Date')\n",
    "    sales_series = store_data.set_index('Date')['Sales']\n",
    "    \n",
    "    # Déterminer les paramètres p, d, q, P, D, Q, s (ici, nous utilisons des valeurs par défaut)\n",
    "    p, d, q = 1, 1, 1\n",
    "    P, D, Q, s = 1, 1, 1, 7  # s=7 pour la saisonnalité hebdomadaire\n",
    "    \n",
    "    model = SARIMAX(sales_series, order=(p, d, q), seasonal_order=(P, D, Q, s), enforce_stationarity=False, enforce_invertibility=False)\n",
    "    sarima_model = model.fit(disp=False)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    joblib.dump(sarima_model, f'models/sarima_model_store_{store_id}.joblib')\n",
    "    return sarima_model\n",
    "\n",
    "# Entraîner pour un magasin (exemple: Store 1)\n",
    "sarima_model = train_sarima(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def train_holt_winters(store_id):\n",
    "    store_data = df[df['Store'] == store_id].sort_values('Date')\n",
    "    sales_series = store_data.set_index('Date')['Sales']\n",
    "    \n",
    "    model = ExponentialSmoothing(sales_series, trend='add', seasonal='add', seasonal_periods=7)\n",
    "    hw_model = model.fit()\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    joblib.dump(hw_model, f'models/holt_winters_model_store_{store_id}.joblib')\n",
    "    return hw_model\n",
    "\n",
    "# Entraîner pour un magasin (exemple: Store 1)\n",
    "hw_model = train_holt_winters(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:51:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "def train_prophet(store_id):\n",
    "    store_data = df[df['Store'] == store_id][['Date', 'Sales']].rename(columns={'Date': 'ds', 'Sales': 'y'})\n",
    "    \n",
    "    model = Prophet()\n",
    "    model.fit(store_data)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    joblib.dump(model, f'models/prophet_model_store_{store_id}.joblib')\n",
    "    return model\n",
    "\n",
    "# Entraîner pour un magasin (exemple: Store 1)\n",
    "prophet_model = train_prophet(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dossier pour les artefacts\n",
    "import os\n",
    "os.makedirs('artefacts', exist_ok=True)\n",
    "\n",
    "def evaluate_and_log_model(model_name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    \n",
    "    # Enregistrer les métriques\n",
    "    with open('artefacts/model_performance.txt', 'a') as f:\n",
    "        f.write(f\"{model_name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2%}\\n\")\n",
    "    \n",
    "    # Enregistrer le graphe des prédictions vs vraies valeurs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_true.values, label='Vraies Valeurs')\n",
    "    plt.plot(y_pred, label='Prédictions')\n",
    "    plt.title(f'Prédictions vs Vraies Valeurs - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'artefacts/{model_name}_predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "lr_results = evaluate_and_log_model('Linear Regression', y_test, y_pred_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "rf_results = evaluate_and_log_model('Random Forest', y_test, y_pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "xgb_results = evaluate_and_log_model('XGBoost', y_test, y_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sales_series \u001b[38;5;241m=\u001b[39m store_data\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m start \u001b[38;5;241m=\u001b[39m sales_series\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m end \u001b[38;5;241m=\u001b[39m df_test[df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m sarima_forecast \u001b[38;5;241m=\u001b[39m sarima_model\u001b[38;5;241m.\u001b[39mpredict(start\u001b[38;5;241m=\u001b[39mstart, end\u001b[38;5;241m=\u001b[39mend)\n\u001b[1;32m      7\u001b[0m y_true_sarima \u001b[38;5;241m=\u001b[39m df_test[df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1714\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1645\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Prédictions pour le magasin 1\n",
    "store_data = df[df['Store'] == 1].sort_values('Date')\n",
    "sales_series = store_data.set_index('Date')['Sales']\n",
    "start = sales_series.index[-1]\n",
    "end = df_test[df_test['Store'] == 1]['Date'].iloc[-1]\n",
    "sarima_forecast = sarima_model.predict(start=start, end=end)\n",
    "y_true_sarima = df_test[df_test['Store'] == 1]['Sales']\n",
    "sarima_results = evaluate_and_log_model('SARIMA (Store 1)', y_true_sarima, sarima_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions pour le magasin 1\n",
    "hw_forecast = hw_model.forecast(steps=len(df_test[df_test['Store'] == 1]))\n",
    "y_true_hw = df_test[df_test['Store'] == 1]['Sales']\n",
    "hw_results = evaluate_and_log_model('Holt-Winters (Store 1)', y_true_hw, hw_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data available for Store 1\n"
     ]
    }
   ],
   "source": [
    "# Préparation des données de test pour Prophet\n",
    "future_dates = df_test[df_test['Store'] == 1]['Date'].reset_index(drop=True)\n",
    "future = pd.DataFrame({'ds': future_dates})\n",
    "\n",
    "# Prédictions\n",
    "forecast = prophet_model.predict(future)\n",
    "y_pred_prophet = forecast['yhat']\n",
    "y_true_prophet = df_test[df_test['Store'] == 1]['Sales'].reset_index(drop=True)\n",
    "\n",
    "# Évaluation\n",
    "prophet_results = evaluate_and_log_model('Prophet (Store 1)', y_true_prophet, y_pred_prophet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model          MAE         RMSE      MAPE\n",
      "0  Linear Regression  1125.635677  1578.452839  0.179172\n",
      "1      Random Forest   675.440375   997.560018  0.105766\n",
      "2            XGBoost   637.729809   940.666691  0.098542\n"
     ]
    }
   ],
   "source": [
    "# Créer un tableau des résultats\n",
    "results_df = pd.DataFrame([\n",
    "    lr_results,\n",
    "    rf_results,\n",
    "    xgb_results,\n",
    "    #sarima_results,\n",
    "    #hw_results,\n",
    "    #prophet_results\n",
    "])\n",
    "\n",
    "# Afficher le tableau\n",
    "print(results_df)\n",
    "\n",
    "# Enregistrer le tableau\n",
    "results_df.to_csv('artefacts/model_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
